#!/bin/bash

# –ü–ï–†–ï–î –ó–ê–ü–£–°–ö–û–ú: –ø–µ—Ä–µ–∫–æ–Ω–∞–π—Å—è, —â–æ —É —Ç–µ–±–µ —î HuggingFace token
# —ñ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤—Å–µ –∑ requirements.txt

# –ù–∞–∑–≤–∞ —Ç–≤–æ—î—ó –º–æ–¥–µ–ª—ñ –Ω–∞ Hugging Face
MODEL_NAME="APAndreyAI/symbios-mistral-lora"
DATA_PATH="./data/training_data.jsonl"
OUTPUT_DIR="./outputs"

# LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–º–æ–∂–Ω–∞ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏ –ø—ñ–¥ —Å–µ–±–µ)
BASE_MODEL="mistralai/Mistral-7B-Instruct-v0.2"
RANK=16
ALPHA=32
EPOCHS=3
LR=5e-5

echo "üîÅ –ó–∞–ø—É—Å–∫–∞—î–º–æ Fine-tune LoRA..."
python train_lora.py \
  --base_model $BASE_MODEL \
  --dataset_path $DATA_PATH \
  --output_dir $OUTPUT_DIR \
  --lora_rank $RANK \
  --lora_alpha $ALPHA \
  --epochs $EPOCHS \
  --learning_rate $LR

if [ $? -ne 0 ]; then
  echo "‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—ñ."
  exit 1
fi

echo "‚úÖ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ü—É—à–∏–º–æ –Ω–∞ HuggingFace..."

huggingface-cli login --token $HF_TOKEN

transformers-cli repo create $MODEL_NAME --type model || true

cd $OUTPUT_DIR
git init
git remote add origin https://huggingface.co/$MODEL_NAME
git checkout -b main
git add .
git commit -m "Initial LoRA push"
git push origin main --force

echo "üöÄ –ú–æ–¥–µ–ª—å $MODEL_NAME —É—Å–ø—ñ—à–Ω–æ –∑–∞–ø—É—à–µ–Ω–∞ –Ω–∞ Hugging Face."
